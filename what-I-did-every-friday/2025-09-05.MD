# Friday study log — 2025-09-05

**Objective**
Fix broken `thumbnail_url` paths to images in my AWS S3 bucket so clients only see valid images.

**Context**
Self-study Friday. Current site shows some images returning S3 `<Code>AccessDenied</Code>`. I have \~7,000 MongoDB documents with `thumbnail_url`.

**Approach (working plan)**

1. Stream every document, `HEAD` each `thumbnail_url` (faster than `GET`).
2. Classify: `OK` (200 + image content-type) vs `BROKEN` (403/404/other).
3. Write a **new** MongoDB collection with the same docs, plus `thumbnail_ok: true/false`.
4. Use the new “clean” collection in staging first; promote when satisfied.
5. Backup before any swap.

**What I worked on today**

* Wrote a validator with controlled concurrency, progress logging, and retries.
* Confirmed handling for redirects and S3 “AccessDenied”.
* Prepared indexes + batch size for efficient streaming.

**Risks / notes**

* S3 failures can be policy-related (Block Public Access, bucket policy/ACL, or wrong key). Broken links may be fixable; don’t delete originals without checking.
* Keep concurrency reasonable to avoid throttling (e.g., 30–75 parallel requests).

**Next steps**

* Run validator against full dataset → produce `items_cleaned_2025_09_05`.
* Point staging to the cleaned collection.
* If many 403s: audit bucket policy + object ACL + key naming (path vs virtual-hosted-style).

---

## Drop-in validator (Node.js, MongoDB)

This streams 7k docs, tests each URL with `HEAD`, and writes a cleaned collection. It never drops your existing DB.

```bash
pnpm i mongodb axios p-limit
```

```js
// validate-thumbs.js
// audit-thumbnails.mjs
// Usage: node audit-thumbnails.mjs records.json
// Env: AWS creds configured; CF_DOMAIN=d341t0j6p2k0ga.cloudfront.net; BUCKET=my-shoe-catalog-bucket
// Config: FIELD=imagePath (dot-path), ASCII_ONLY=false

import fs from "fs";
import { S3Client, HeadObjectCommand, CopyObjectCommand, DeleteObjectCommand } from "@aws-sdk/client-s3";
import path from "path";
import { pipeline } from "stream";
import { once } from "events";
import http from "http";
import https from "https";

// ---------- CONFIG ----------
const CF_DOMAIN = process.env.CF_DOMAIN; // e.g. d341t0j6p2k0ga.cloudfront.net
const BUCKET    = process.env.BUCKET;    // your S3 bucket
const FIELD     = process.env.FIELD || "imagePath"; // dot-path to the key in each record
const ASCII_ONLY = process.env.ASCII_ONLY === "true"; // force ASCII slugs?
const DO_RENAME = process.env.DO_RENAME === "true";   // copy+delete to new keys
// ----------------------------

if (!CF_DOMAIN || !BUCKET) {
  console.error("Set CF_DOMAIN and BUCKET env vars.");
  process.exit(1);
}
const inputFile = process.argv[2];
if (!inputFile) {
  console.error("Usage: node audit-thumbnails.mjs records.json");
  process.exit(1);
}

function getByDotPath(obj, dot) {
  return dot.split(".").reduce((a, k) => (a && a[k] !== undefined ? a[k] : undefined), obj);
}
function setByDotPath(obj, dot, val) {
  const parts = dot.split(".");
  const last = parts.pop();
  const parent = parts.reduce((a, k) => (a[k] = a[k] ?? {}), obj);
  parent[last] = val;
}
function toNFC(s) {
  try { return s.normalize("NFC"); } catch { return s; }
}
function mapFullWidthToASCII(s) {
  return s.replace(/\uFF08/g, "(").replace(/\uFF09/g, ")").replace(/\u3000/g, " ");
}
function sanitizeSegment(seg) {
  seg = toNFC(mapFullWidthToASCII(seg)).trim();
  seg = seg.replace(/\s+/g, "-");
  if (ASCII_ONLY) {
    seg = seg.normalize("NFD").replace(/[\u0300-\u036f]/g, "");
    seg = seg.replace(/[^\w.-]+/g, "-");
  } else {
    seg = seg.replace(/[?#%<>[\]^`{|}\\]/g, "-");
  }
  return seg.replace(/-+/g, "-");
}
function canonicalizeKey(key) {
  return key.split("/").map(sanitizeSegment).join("/");
}
function encPathForUrl(p) {
  return p.split("/").map(encodeURIComponent).join("/");
}
async function httpHead(url) {
  return new Promise((resolve) => {
    const h = url.startsWith("https:") ? https : http;
    const req = h.request(url, { method: "HEAD", timeout: 8000 }, (res) => {
      resolve({ ok: res.statusCode && res.statusCode >= 200 && res.statusCode < 400, status: res.statusCode });
    });
    req.on("error", () => resolve({ ok: false, status: 0 }));
    req.on("timeout", () => { req.destroy(); resolve({ ok: false, status: 0 }); });
    req.end();
  });
}

const s3 = new S3Client({});

async function s3Exists(Key) {
  try {
    await s3.send(new HeadObjectCommand({ Bucket: BUCKET, Key }));
    return true;
  } catch {
    return false;
  }
}

function uniq(arr) { return [...new Set(arr)]; }

async function main() {
  const raw = fs.readFileSync(inputFile, "utf8");
  const records = JSON.parse(raw);

  const reportRows = [];
  const redirects = [];
  const dbPatch = [];

  for (const rec of records) {
    const id = rec.id ?? rec._id ?? rec.uuid ?? "(no-id)";
    const keyRaw = getByDotPath(rec, FIELD);
    if (!keyRaw || typeof keyRaw !== "string") {
      reportRows.push([id, "", "no", "no", "", "", "", "SKIP", "missing path"]);
      continue;
    }
    // Candidates to try
    const candidates = uniq([
      keyRaw,
      toNFC(keyRaw),
      mapFullWidthToASCII(keyRaw),
      toNFC(mapFullWidthToASCII(keyRaw)),
      canonicalizeKey(keyRaw),
    ]);

    // Probe S3
    let foundKey = null;
    for (const k of candidates) {
      if (await s3Exists(k)) { foundKey = k; break; }
    }

    // Probe CloudFront with encodeURI once
    const urlOriginal = `https://${CF_DOMAIN}/${encPathForUrl(keyRaw)}`;
    const cf = await httpHead(urlOriginal);

    let action = "KEEP";
    let reason = "";
    let suggestedKey = "";
    let suggestedUrl = "";

    if (foundKey) {
      if (foundKey !== keyRaw) {
        // DB path doesn't match actual S3 key: suggest DB update
        action = "UPDATE_DB";
        reason = "db path != s3 key";
        suggestedKey = foundKey;
        suggestedUrl = `https://${CF_DOMAIN}/${encPathForUrl(foundKey)}`;
        dbPatch.push({ id, newPath: foundKey });
      } else {
        // Exact match in S3; ensure canonical policy
        const canon = canonicalizeKey(foundKey);
        if (canon !== foundKey) {
          action = DO_RENAME ? "RENAME" : "SUGGEST_RENAME";
          reason = "non-canonical key";
          suggestedKey = canon;
          suggestedUrl = `https://${CF_DOMAIN}/${encPathForUrl(canon)}`;
          redirects.push({ oldKey: foundKey, newKey: canon });
        }
      }
    } else {
      // Not in S3 under any candidate; if CF works, URL OK but key unknown (edge case)
      if (cf.ok) {
        action = "KEEP";
        reason = "served by edge (check origin path rules)";
      } else {
        // Try canonical guess; if missing, recommend re-upload or rename if a near-miss is detectable
        const guess = canonicalizeKey(keyRaw);
        suggestedKey = guess;
        suggestedUrl = `https://${CF_DOMAIN}/${encPathForUrl(guess)}`;
        action = "MISSING";
        reason = "no s3 object found";
      }
    }

    reportRows.push([
      id,
      keyRaw,
      foundKey ? "yes" : "no",
      cf.ok ? `yes(${cf.status})` : "no",
      urlOriginal,
      suggestedKey,
      suggestedUrl,
      action,
      reason
    ]);
  }

  // Write outputs
  fs.writeFileSync("report.csv",
    "id,originalPath,s3Found,cfFound,originalUrl,suggestedKey,suggestedUrl,action,reason\n" +
    reportRows.map(r => r.map(v => `"${String(v).replace(/"/g,'""')}"`).join(",")).join("\n"),
    "utf8"
  );
  fs.writeFileSync("redirects.csv",
    "oldKey,newKey\n" + redirects.map(x => `"${x.oldKey.replace(/"/g,'""')}","${x.newKey.replace(/"/g,'""')}"`).join("\n"),
    "utf8"
  );
  fs.writeFileSync("db_patch.json", JSON.stringify(dbPatch, null, 2), "utf8");

  console.log(`Wrote report.csv, redirects.csv, db_patch.json`);
  if (DO_RENAME && redirects.length) {
    console.log(`DO_RENAME=true -> applying ${redirects.length} S3 renames...`);
    for (const { oldKey, newKey } of redirects) {
      try {
        await s3.send(new CopyObjectCommand({
          Bucket: BUCKET,
          CopySource: `/${BUCKET}/${encodeURIComponent(oldKey)}`,
          Key: newKey,
          MetadataDirective: "COPY"
        }));
        await s3.send(new DeleteObjectCommand({ Bucket: BUCKET, Key: oldKey }));
        console.log(`RENAMED ${oldKey} -> ${newKey}`);
      } catch (e) {
        console.error(`FAILED ${oldKey} -> ${newKey}:`, e?.name || e?.message || e);
      }
    }
  }
}

main().catch(e => { console.error(e); process.exit(1); });

