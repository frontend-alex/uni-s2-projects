# Friday study log — 2025-09-05

**Objective**
Fix broken `thumbnail_url` paths to images in my AWS S3 bucket so clients only see valid images.

**Context**
Self-study Friday. Current site shows some images returning S3 `<Code>AccessDenied</Code>`. I have \~7,000 MongoDB documents with `thumbnail_url`.

**Approach (working plan)**

1. Stream every document, `HEAD` each `thumbnail_url` (faster than `GET`).
2. Classify: `OK` (200 + image content-type) vs `BROKEN` (403/404/other).
3. Write a **new** MongoDB collection with the same docs, plus `thumbnail_ok: true/false`.
4. Use the new “clean” collection in staging first; promote when satisfied.
5. Backup before any swap.

**What I worked on today**

* Wrote a validator with controlled concurrency, progress logging, and retries.
* Confirmed handling for redirects and S3 “AccessDenied”.
* Prepared indexes + batch size for efficient streaming.

**Risks / notes**

* S3 failures can be policy-related (Block Public Access, bucket policy/ACL, or wrong key). Broken links may be fixable; don’t delete originals without checking.
* Keep concurrency reasonable to avoid throttling (e.g., 30–75 parallel requests).

**Next steps**

* Run validator against full dataset → produce `items_cleaned_2025_09_05`.
* Point staging to the cleaned collection.
* If many 403s: audit bucket policy + object ACL + key naming (path vs virtual-hosted-style).

---

## Drop-in validator (Node.js, MongoDB)

This streams 7k docs, tests each URL with `HEAD`, and writes a cleaned collection. It never drops your existing DB.

```bash
npm i mongodb axios p-limit
```

```js
// validate-thumbs.js
import { MongoClient } from "mongodb";
import axios from "axios";
import pLimit from "p-limit";

const MONGO_URI = process.env.MONGO_URI;     // mongodb+srv://...
const DB_NAME   = process.env.DB_NAME;       // e.g. "mydb"
const SRC_COL   = process.env.SRC_COL;       // e.g. "items"
const DST_COL   = process.env.DST_COL || `items_cleaned_${new Date().toISOString().slice(0,10)}`;
const CONCURRENCY = parseInt(process.env.CONCURRENCY || "50", 10);
const TIMEOUT_MS  = parseInt(process.env.TIMEOUT_MS || "8000", 10);
const RETRIES     = parseInt(process.env.RETRIES || "1", 10);

const client = new MongoClient(MONGO_URI, { maxPoolSize: 20 });

async function headOk(url) {
  if (!url) return { ok: false, status: 0, reason: "missing" };
  let lastErr = null;
  for (let attempt = 0; attempt <= RETRIES; attempt++) {
    try {
      const res = await axios.head(url, {
        maxRedirects: 3,
        timeout: TIMEOUT_MS,
        validateStatus: () => true, // we’ll interpret ourselves
      });
      const ct = (res.headers["content-type"] || "").toLowerCase();
      const isImg = ct.includes("image/");
      const ok = res.status === 200 && isImg;
      return { ok, status: res.status, contentType: ct };
    } catch (err) {
      lastErr = err;
    }
  }
  return { ok: false, status: 0, reason: lastErr?.code || "network_error" };
}

async function run() {
  await client.connect();
  const db  = client.db(DB_NAME);
  const src = db.collection(SRC_COL);
  const dst = db.collection(DST_COL);

  // keep destination clean for re-runs
  await dst.deleteMany({});

  // helpful index for fast lookups if you re-run
  await dst.createIndex({ _id: 1 });
  await dst.createIndex({ thumbnail_ok: 1 });

  const limit = pLimit(CONCURRENCY);
  const cursor = src.find({}, { batchSize: 500, projection: { /* select fields you need */ } });

  let processed = 0, okCount = 0, brokenCount = 0;
  const pending = [];
  while (await cursor.hasNext()) {
    const doc = await cursor.next();
    pending.push(limit(async () => {
      const url = doc.thumbnail_url;
      const check = await headOk(url);

      const out = {
        ...doc,
        thumbnail_ok: check.ok,
        thumbnail_status: check.status || null,
        thumbnail_reason: check.reason || null,
        thumbnail_content_type: check.contentType || null,
        validated_at: new Date(),
      };

      await dst.replaceOne({ _id: doc._id }, out, { upsert: true });

      processed++;
      if (check.ok) okCount++; else brokenCount++;
      if (processed % 250 === 0) {
        console.log(`Processed ${processed} — OK: ${okCount}, BROKEN: ${brokenCount}`);
      }
    }));
  }
  await Promise.all(pending);

  console.log(`Done. Processed: ${processed}, OK: ${okCount}, BROKEN: ${brokenCount}`);
  await client.close();
}

run().catch(err => {
  console.error(err);
  process.exit(1);
});
```

**Run it**

```bash
export MONGO_URI="mongodb+srv://..."
export DB_NAME="mydb"
export SRC_COL="items"
# optional overrides:
export DST_COL="items_cleaned_2025-09-05"
export CONCURRENCY=50
node validate-thumbs.js
```

### Querying results

* Count broken:

```js
db.items_cleaned_2025_09_05.countDocuments({ thumbnail_ok: false })
```

* Use only valid in staging:

```js
db.items_cleaned_2025_09_05.find({ thumbnail_ok: true })
```

### Swap strategy (no downtime)

1. **Backup**: `mongodump` the original collection.
2. Point your staging site to `items_cleaned_YYYY-MM-DD`.
3. Smoke test pages and random samples.
4. Promote by changing the collection name in prod config, or atomically rename collections:

   * `items` → `items_backup_YYYYMMDD`
   * `items_cleaned_YYYYMMDD` → `items`

---

## Extra S3 sanity checks (the usual culprits)

* **Block Public Access**: bucket-level setting may be blocking object access even if the object is public.
* **Bucket policy / ACL**: ensure `s3:GetObject` allowed for the key prefix you use.
* **Key paths**: confirm your app’s URL format (virtual-hosted vs path-style). A subtle prefix mismatch (`/thumbs/` vs `/thumbnails/`) causes 403 or 404.
* **CORS**: only affects browser reads, but worth aligning for previews.

If you want, I can turn today’s log into a recurring “Friday study log” template and ping you every Friday morning to fill it in.
